{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYinppe_V__V",
        "outputId": "8c357de1-532e-42ad-ba8a-165a2644fc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chaos PSO SVM Based Fault Detection Metrics\n",
            "Iteration 1/20, Best Score: 0.9652173913043478\n",
            "Iteration 2/20, Best Score: 0.9652173913043478\n",
            "Iteration 3/20, Best Score: 0.9652173913043478\n",
            "Iteration 4/20, Best Score: 0.967391304347826\n",
            "Iteration 5/20, Best Score: 0.967391304347826\n",
            "Iteration 6/20, Best Score: 0.967391304347826\n",
            "Iteration 7/20, Best Score: 0.967391304347826\n",
            "Iteration 8/20, Best Score: 0.967391304347826\n",
            "Iteration 9/20, Best Score: 0.967391304347826\n",
            "Iteration 10/20, Best Score: 0.967391304347826\n",
            "Iteration 11/20, Best Score: 0.967391304347826\n",
            "Iteration 12/20, Best Score: 0.967391304347826\n",
            "Iteration 13/20, Best Score: 0.967391304347826\n",
            "Iteration 14/20, Best Score: 0.967391304347826\n",
            "Iteration 15/20, Best Score: 0.967391304347826\n",
            "Iteration 16/20, Best Score: 0.967391304347826\n",
            "Iteration 17/20, Best Score: 0.967391304347826\n",
            "Iteration 18/20, Best Score: 0.967391304347826\n",
            "Iteration 19/20, Best Score: 0.967391304347826\n",
            "Iteration 20/20, Best Score: 0.967391304347826\n",
            "Accuracy: 96.74%\n",
            "Objective function value: 0.967391304347826\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        46\n",
            "           1       0.98      0.90      0.94        50\n",
            "           2       0.98      0.92      0.95        49\n",
            "           3       0.97      1.00      0.99        37\n",
            "           4       1.00      1.00      1.00        47\n",
            "           5       0.95      1.00      0.97        39\n",
            "           6       0.97      1.00      0.99        39\n",
            "           7       1.00      1.00      1.00        54\n",
            "           8       0.91      0.96      0.94        54\n",
            "           9       0.98      0.98      0.98        45\n",
            "\n",
            "    accuracy                           0.97       460\n",
            "   macro avg       0.97      0.97      0.97       460\n",
            "weighted avg       0.97      0.97      0.97       460\n",
            "\n",
            "Genetic Algorithm - Gaussian Naive Bayes Based Fault Detection Metrics\n",
            "Generation 0, Best Fitness: 0.8768115942028986\n",
            "Generation 1, Best Fitness: 0.9094202898550725\n",
            "Generation 2, Best Fitness: 0.9130434782608695\n",
            "Generation 3, Best Fitness: 0.9166666666666666\n",
            "Generation 4, Best Fitness: 0.9202898550724637\n",
            "Generation 5, Best Fitness: 0.9239130434782609\n",
            "Generation 6, Best Fitness: 0.9257246376811594\n",
            "Generation 7, Best Fitness: 0.927536231884058\n",
            "Generation 8, Best Fitness: 0.9293478260869565\n",
            "Generation 9, Best Fitness: 0.9293478260869565\n",
            "Generation 10, Best Fitness: 0.9293478260869565\n",
            "Generation 11, Best Fitness: 0.9293478260869565\n",
            "Generation 12, Best Fitness: 0.9293478260869565\n",
            "Generation 13, Best Fitness: 0.9329710144927537\n",
            "Generation 14, Best Fitness: 0.9329710144927537\n",
            "Generation 15, Best Fitness: 0.9347826086956522\n",
            "Generation 16, Best Fitness: 0.9347826086956522\n",
            "Generation 17, Best Fitness: 0.9347826086956522\n",
            "Generation 18, Best Fitness: 0.9347826086956522\n",
            "Generation 19, Best Fitness: 0.9365942028985508\n",
            "Accuracy: 93.04%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        46\n",
            "           1       0.93      0.78      0.85        50\n",
            "           2       1.00      0.76      0.86        49\n",
            "           3       0.97      1.00      0.99        37\n",
            "           4       1.00      1.00      1.00        47\n",
            "           5       0.95      0.97      0.96        39\n",
            "           6       1.00      1.00      1.00        39\n",
            "           7       1.00      0.98      0.99        54\n",
            "           8       0.72      0.94      0.82        54\n",
            "           9       0.94      0.98      0.96        45\n",
            "\n",
            "    accuracy                           0.93       460\n",
            "   macro avg       0.94      0.93      0.94       460\n",
            "weighted avg       0.94      0.93      0.93       460\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "def logistic_map(x, r=4):\n",
        "    \"\"\"Logistic map function for generating chaotic sequences.\"\"\"\n",
        "    return r * x * (1 - x)\n",
        "\n",
        "# Load and preprocess the CWRU bearing dataset\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Assuming the last column is the target variable\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Convert labels to numeric if necessary\n",
        "    if y.dtype == 'object' or isinstance(y[0], str):\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def objective_function(params, X_train, y_train, X_test, y_test, is_class_report_needed=False):\n",
        "    \"\"\"Objective function for the SVM model.\"\"\"\n",
        "    C, gamma = params\n",
        "    model = SVC(C=C, gamma=gamma)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    if is_class_report_needed:\n",
        "        class_report = classification_report(y_test, predictions, labels=np.unique(predictions))\n",
        "        return -accuracy, class_report  # Negative because we want to maximize accuracy\n",
        "    else:\n",
        "        return -accuracy\n",
        "\n",
        "def chaos_pso(file_path, num_particles=30, num_dimensions=2, num_iterations=20):\n",
        "    \"\"\"Chaos Particle Swarm Optimization (CPSO) algorithm.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Initialize particle swarm optimization parameters\n",
        "    w = 0.7  # Inertia weight\n",
        "    c1 = 1.5  # Cognitive (particle) weight\n",
        "    c2 = 1.5  # Social (swarm) weight\n",
        "\n",
        "    # Initialize the positions and velocities of the particles\n",
        "    positions = np.random.uniform([0.1, 0.0001], [10, 1], (num_particles, num_dimensions))\n",
        "    velocities = np.random.uniform(-1, 1, (num_particles, num_dimensions))\n",
        "    personal_best_positions = np.copy(positions)\n",
        "    #print(\"Computing best scores\")\n",
        "    personal_best_scores = np.apply_along_axis(lambda pos: objective_function(pos, X_train, y_train, X_test, y_test), 1, positions)\n",
        "\n",
        "    #print(\"personal_best_scores\", personal_best_scores)\n",
        "    # Find the global best position\n",
        "    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
        "    global_best_score = np.min(personal_best_scores)\n",
        "\n",
        "    global_class_report = None\n",
        "    #print(\"global_best_score\", global_best_score)\n",
        "    # Initialize the chaotic sequence\n",
        "    chaotic_sequence = np.random.rand(num_particles)\n",
        "\n",
        "    # Start the optimization process\n",
        "    for t in range(num_iterations):\n",
        "        for i in range(num_particles):\n",
        "            # Update chaotic sequence\n",
        "            chaotic_sequence[i] = logistic_map(chaotic_sequence[i])\n",
        "\n",
        "            # Update velocities\n",
        "            r1 = chaotic_sequence[i]\n",
        "            r2 = 1 - chaotic_sequence[i]  # Use the complement for the second random number\n",
        "            velocities[i] = (w * velocities[i] +\n",
        "                             c1 * r1 * (personal_best_positions[i] - positions[i]) +\n",
        "                             c2 * r2 * (global_best_position - positions[i]))\n",
        "\n",
        "            # Update positions\n",
        "            positions[i] = positions[i] + velocities[i]\n",
        "\n",
        "            # Boundary constraints\n",
        "            positions[i] = np.clip(positions[i], [0.1, 0.0001], [10, 1])\n",
        "\n",
        "            # Evaluate new position\n",
        "            score, class_report = objective_function(positions[i], X_train, y_train, X_test, y_test, True)\n",
        "\n",
        "            # Update personal best\n",
        "            if score < personal_best_scores[i]:\n",
        "                personal_best_positions[i] = positions[i]\n",
        "                personal_best_scores[i] = score\n",
        "\n",
        "                # Update global best\n",
        "                if score < global_best_score or global_class_report is None:\n",
        "                    global_best_position = positions[i]\n",
        "                    global_best_score = score\n",
        "                    global_class_report = class_report\n",
        "\n",
        "        print(f\"Iteration {t+1}/{num_iterations}, Best Score: {-global_best_score}\")\n",
        "    accuracy = abs(global_best_score)\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f\"Objective function value: {-global_best_score}\")\n",
        "    print(\"Classification report:\")\n",
        "    print(global_class_report)\n",
        "\n",
        "def normalize_data(X_train, X_test):\n",
        "    \"\"\"Normalize the input data.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "def apply_pca(X_train, X_test, n_components=0.999):\n",
        "    \"\"\"Apply PCA for dimensionality reduction.\"\"\"\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    return X_train_pca, X_test_pca\n",
        "\n",
        "def initialize_population(pop_size, num_features):\n",
        "    \"\"\"Initialize the population for the genetic algorithm.\"\"\"\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "def fitness(individual, X, y):\n",
        "    \"\"\"Evaluate the fitness of an individual.\"\"\"\n",
        "    selected_features = np.where(individual == 1)[0]\n",
        "    if len(selected_features) == 0:\n",
        "        return 0\n",
        "    X_selected = X[:, selected_features]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(X_train, y_train)\n",
        "    y_pred = gnb.predict(X_val)\n",
        "    return accuracy_score(y_val, y_pred)\n",
        "\n",
        "def select_parents(population, fitnesses, num_parents):\n",
        "    \"\"\"Select parents for the next generation.\"\"\"\n",
        "    parents = np.empty((num_parents, population.shape[1]), dtype=int)\n",
        "    for parent_num in range(num_parents):\n",
        "        max_fitness_idx = np.where(fitnesses == np.max(fitnesses))\n",
        "        max_fitness_idx = max_fitness_idx[0][0]\n",
        "        parents[parent_num, :] = population[max_fitness_idx, :]\n",
        "        fitnesses[max_fitness_idx] = -999999\n",
        "    return parents\n",
        "\n",
        "def crossover(parents, offspring_size):\n",
        "    \"\"\"Perform crossover between parents to generate offspring.\"\"\"\n",
        "    offspring = np.empty(offspring_size, dtype=int)\n",
        "    crossover_point = np.uint8(offspring_size[1]/2)\n",
        "    for k in range(offspring_size[0]):\n",
        "        parent1_idx = k % parents.shape[0]\n",
        "        parent2_idx = (k+1) % parents.shape[0]\n",
        "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
        "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
        "    return offspring\n",
        "\n",
        "def mutation(offspring_crossover, mutation_rate):\n",
        "    \"\"\"Mutate the offspring by flipping bits.\"\"\"\n",
        "    for idx in range(offspring_crossover.shape[0]):\n",
        "        for _ in range(mutation_rate):\n",
        "            gene_idx = np.random.randint(0, offspring_crossover.shape[1])\n",
        "            offspring_crossover[idx, gene_idx] = 1 - offspring_crossover[idx, gene_idx]\n",
        "    return offspring_crossover\n",
        "\n",
        "def genetic_algorithm(X_train_scaled, y_train, num_generations=20, pop_size=20, num_parents_mating=10, mutation_rate=1):\n",
        "    \"\"\"Run the genetic algorithm for feature selection.\"\"\"\n",
        "    num_features = X_train_scaled.shape[1]\n",
        "    population = initialize_population(pop_size, num_features)\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitnesses = np.array([fitness(ind, X_train_scaled, y_train) for ind in population])\n",
        "        parents = select_parents(population, fitnesses, num_parents_mating)\n",
        "        offspring_crossover = crossover(parents, (pop_size - num_parents_mating, num_features))\n",
        "        offspring_mutation = mutation(offspring_crossover, mutation_rate)\n",
        "        population[0:num_parents_mating, :] = parents\n",
        "        population[num_parents_mating:, :] = offspring_mutation\n",
        "        print(f'Generation {generation}, Best Fitness: {np.max(fitnesses)}')\n",
        "\n",
        "    best_solution_idx = np.argmax(fitnesses)\n",
        "    best_solution = population[best_solution_idx, :]\n",
        "    selected_features = np.where(best_solution == 1)[0]\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "def gaussian_naive_bayes(X_train, X_test, y_train, y_test, selected_features):\n",
        "    \"\"\"Train and evaluate the Gaussian Naive Bayes model.\"\"\"\n",
        "    X_train_ga = X_train[:, selected_features]\n",
        "    X_test_ga = X_test[:, selected_features]\n",
        "\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(X_train_ga, y_train)\n",
        "    y_pred = gnb.predict(X_test_ga)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "def main():\n",
        "    file_path = \"/content/feature_time_48k_2048_load_1.csv\"\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path)\n",
        "    print(\"Chaos PSO SVM Based Fault Detection Metrics\")\n",
        "    chaos_pso(file_path)\n",
        "\n",
        "    print(\"Genetic Algorithm - Gaussian Naive Bayes Based Fault Detection Metrics\")\n",
        "    selected_features = genetic_algorithm(X_train, y_train)\n",
        "    gaussian_naive_bayes(X_train, X_test, y_train, y_test, selected_features)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}